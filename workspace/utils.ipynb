{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "\n",
    "In this section, we import various libraries that are essential for data processing, model building, and evaluation.\n",
    "\n",
    "- **pandas**: A powerful data manipulation library for Python. It is used for reading data, data manipulation, and data analysis.\n",
    "- **numpy**: A fundamental package for scientific computing with Python. It is used for array operations and mathematical functions.\n",
    "- **os**: A library that provides a way of using operating system dependent functionality like reading or writing to the file system.\n",
    "- **warnings**: A library to handle warnings in Python, allowing us to ignore them if necessary.\n",
    "- **lightgbm**: LightGBM is a gradient boosting framework that uses tree-based learning algorithms. It is designed for fast performance and efficiency.\n",
    "- **xgboost**: XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable.\n",
    "- **catboost**: CatBoost is a gradient boosting library that supports categorical features directly and is designed to be fast and easy to use.\n",
    "- **optuna**: A hyperparameter optimization framework to automate hyperparameter search.\n",
    "- **sklearn.preprocessing**: Contains preprocessing utilities and transformers that are used to transform data before feeding it into the model.\n",
    "  - **MinMaxScaler**: Scales and translates each feature individually such that it is in the given range on the training set.\n",
    "  - **StandardScaler**: Standardizes features by removing the mean and scaling to unit variance.\n",
    "  - **RobustScaler**: Scales features using statistics that are robust to outliers.\n",
    "  - **QuantileTransformer**: Transforms features using quantile information to map data to a uniform distribution.\n",
    "  - **LabelEncoder**: Encodes target labels with value between 0 and n_classes-1.\n",
    "- **sklearn.metrics**: Provides metrics to evaluate the performance of models.\n",
    "  - **mean_absolute_error**: Computes the mean absolute error between the ground truth and predicted values.\n",
    "- **sklearn.model_selection**: Contains utilities to split the dataset into training and testing subsets and perform cross-validation.\n",
    "  - **TimeSeriesSplit**: Provides train/test indices to split time series data samples sequentially.\n",
    "- **sklearn.ensemble**: Contains ensemble-based methods for regression and classification.\n",
    "  - **HistGradientBoostingRegressor**: A gradient boosting regressor for large datasets that uses histogram-based algorithm.\n",
    "\n",
    "Below is the code that imports these libraries and sets up some initial configurations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.max_rows\", 200)\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as catb\n",
    "import optuna\n",
    "optuna.logging.disable_default_handler()\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, QuantileTransformer, LabelEncoder\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing Function\n",
    "\n",
    "This section contains a function to preprocess the date-related data in the dataframe. The function, `process_date`, performs the following operations:\n",
    "\n",
    "1. **Convert Date Column**: Converts the 'tarih' column to datetime format.\n",
    "2. **Sort Data**: Sorts the dataframe by 'ilce' (district) and 'tarih' (date) in ascending order.\n",
    "3. **Set Index**: Sets the 'tarih' column as the index of the dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_date(dataframe):\n",
    "    dataframe['tarih'] = pd.to_datetime(dataframe['tarih'], format='%Y-%m-%d')\n",
    "    dataframe.sort_values(by=['ilce', 'tarih'], ascending=True, inplace=True)\n",
    "    dataframe.set_index('tarih', inplace=True)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Concatenation Function\n",
    "\n",
    "This section contains a function to ensure that each district (`ilce`) in the dataframe has a complete date range. The function, `concatenate`, performs the following operations:\n",
    "\n",
    "1. **Create Complete Date Ranges**: For each district, create a complete date range from the minimum to the maximum date present in the data.\n",
    "2. **Merge Dataframes**: Merge the original dataframe with the complete date range dataframe for each district, filling in missing values.\n",
    "3. **Concatenate Dataframes**: Concatenate all district-specific dataframes into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(dataframe):\n",
    "    all_dates_dfs = {}\n",
    "    for ilce, ilce_df in dataframe.groupby(['ilce']):\n",
    "        min_date = ilce_df.index.min()\n",
    "        max_date = ilce_df.index.max()\n",
    "        date_range = pd.date_range(start=min_date, end=max_date)\n",
    "        all_dates_dfs[ilce] = pd.DataFrame(index=date_range)\n",
    "\n",
    "    merged_dfs = {}\n",
    "    for ilce, ilce_df in dataframe.groupby('ilce'):\n",
    "        merged_dfs[ilce] = pd.merge(all_dates_dfs[ilce], ilce_df, how='left', left_index=True, right_index=True)\n",
    "        merged_dfs[ilce].fillna({'ilce': ilce, 'bildirimsiz_sum': 0, 'bildirimli_sum': 0}, inplace=True)\n",
    "\n",
    "    dataframe = pd.concat(merged_dfs.values())\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Grouping Function\n",
    "\n",
    "This section contains a function to group the dataframe by district (`ilce`), year, month, and day, and then aggregate the sum of specific columns. The function, `process_group`, performs the following operations:\n",
    "\n",
    "1. **Extract Date Components**: Extracts the year, month, and day from the index.\n",
    "2. **Group by Columns**: Groups the dataframe by `ilce`, `year`, `month`, and `day`.\n",
    "3. **Aggregate Sum**: Aggregates the sum of `bildirimsiz_sum` and `bildirimli_sum` columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_group(dataframe):\n",
    "    dataframe['year'] = dataframe.index.year\n",
    "    dataframe['month'] = dataframe.index.month\n",
    "    dataframe['day'] = dataframe.index.day\n",
    "    dataframe = dataframe.groupby(['ilce', 'year', 'month', 'day'], as_index=False)['bildirimsiz_sum', 'bildirimli_sum'].agg('sum')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Feature Extraction Function\n",
    "\n",
    "This section contains a function to extract and create new features from the existing weather data in the dataframe. The function, `extract_features_on_weather`, performs the following operations:\n",
    "\n",
    "1. **Calculate Temperature Difference**: Computes the difference between the actual temperature (`t_2m:C`) and the apparent temperature (`t_apparent:C`).\n",
    "2. **Calculate Wind Direction Components**: Computes the sine and cosine components of the wind direction (`wind_dir_10m:d`).\n",
    "3. **Calculate Wind Speed Components**: Computes the wind speed components based on the sine and cosine of the wind direction.\n",
    "\n",
    "### Feature Descriptions\n",
    "- `temp_diff`: The difference between the actual temperature (`t_2m:C`) and the apparent temperature (`t_apparent:C`).\n",
    "- `sin_direction`: The sine of the wind direction (`wind_dir_10m:d`).\n",
    "- `cos_direction`: The cosine of the wind direction (`wind_dir_10m:d`).\n",
    "- `WindSpeed_sin_component`: The wind speed component in the sine direction.\n",
    "- `WindSpeed_cos_component`: The wind speed component in the cosine direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_on_weather(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    # dataframe[\"wind_coolinf_effect\"] = 33 - (10 * (dataframe[\"wind_speed_10m:ms\"]**0.5)) + (10.45 * dataframe[\"t_2m:C\"]) - (0.2778 * (dataframe[\"wind_speed_10m:ms\"] * dataframe[\"t_2m:C\"]))\n",
    "    # dataframe['temperature_relative_humidity_interaction'] = dataframe['t_2m:C'] * (1 + 0.33 * dataframe['relative_humidity_2m:p'] / 100)\n",
    "    # dataframe[\"sunlight_intensity\"] = dataframe[\"global_rad:W\"] / (1 + dataframe[\"effective_cloud_cover:p\"])\n",
    "    # dataframe[\"wind_chill_index\"] = 13.12 + 0.6215 * dataframe[\"t_2m:C\"] - 11.37 * (dataframe[\"wind_speed_10m:ms\"] ** 0.16) + 0.3965 * dataframe[\"t_2m:C\"] * (dataframe[\"wind_speed_10m:ms\"] ** 0.16)\n",
    "    # dataframe['precipitation_cloud_interaction'] = dataframe['prob_precip_1h:p'] * (1 + 0.5 * dataframe['effective_cloud_cover:p'] / 100)\n",
    "    # dataframe['temperature_ratio'] = dataframe['t_apparent:C'] / (dataframe['t_2m:C'] + 1e-10)\n",
    "    dataframe['temp_diff'] = (dataframe['t_2m:C'] - dataframe['t_apparent:C'])\n",
    "    dataframe['sin_direction'] = np.sin(dataframe['wind_dir_10m:d']*np.pi/180)\n",
    "    dataframe['cos_direction'] = np.cos(dataframe['wind_dir_10m:d']*np.pi/180)\n",
    "    dataframe[\"WindSpeed_sin_component\"] = dataframe['wind_speed_10m:ms'] * dataframe[\"sin_direction\"]\n",
    "    dataframe[\"WindSpeed_cos_component\"] = dataframe['wind_speed_10m:ms'] * dataframe[\"cos_direction\"]\n",
    "    # dataframe['is_rainy'] = np.where(dataframe['prob_precip_1h:p'] > 0, 1, 0)\n",
    "    # dataframe['is_windy'] = np.where(dataframe['wind_speed_10m:ms'] > 10, 1, 0)\n",
    "    # dataframe['log_wind_speed'] = np.log(dataframe['wind_speed_10m:ms'] + 1)\n",
    "    dataframe.drop(['cos_direction', 't_2m:C', 't_apparent:C'], axis=1, inplace=True)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data Processing Function\n",
    "\n",
    "This section contains a function to process and aggregate weather data. The function, `weather_df_process`, performs the following operations:\n",
    "\n",
    "1. **Extract Date Components**: Extracts the year, month, and day from the index.\n",
    "2. **Drop Unnecessary Columns**: Drops the 'lat' and 'lon' columns as they are not needed for further processing.\n",
    "3. **Group and Aggregate Data**: Groups the dataframe by `ilce`, year, month, day, and daily frequency, and then aggregates the mean, minimum, and maximum values for each group.\n",
    "4. **Rename Columns**: Renames the columns to include the aggregation type for clarity.\n",
    "\n",
    "### Column Descriptions\n",
    "- The new columns will include the original column names suffixed with '_mean', '_min', and '_max' to indicate the type of aggregation performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_df_process(dataframe):\n",
    "    dataframe['year'] = dataframe.index.year\n",
    "    dataframe['month'] = dataframe.index.month\n",
    "    dataframe['day'] = dataframe.index.day\n",
    "    dataframe.drop(columns=['lat', 'lon'], axis=1, inplace=True)\n",
    "    weather_df = dataframe.groupby(['ilce', 'year', 'month', 'day', pd.Grouper(freq='D')]).agg(['mean', 'min', 'max'])\n",
    "    weather_df.columns = [f'{col}_{stat}' for col, stat in weather_df.columns]\n",
    "\n",
    "    return weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Merging Function\n",
    "\n",
    "This section contains a function to merge the main dataframe with weather data and holidays data. The function, `merge_data`, performs the following operations:\n",
    "\n",
    "1. **Merge with Weather Data**: Merges the main dataframe with the weather dataframe on `ilce`, `year`, `month`, and `day` using an inner join.\n",
    "2. **Merge with Holidays Data**: Merges the resulting dataframe with the holidays dataframe on `year`, `month`, and `day` using a left join.\n",
    "3. **Sort Data**: Sorts the dataframe by `ilce`, `year`, `month`, and `day` in ascending order.\n",
    "4. **Set Date Index**: Converts the year, month, and day columns into a datetime format and sets it as the index.\n",
    "5. **Handle Missing Holiday Data**: Fills missing values in the 'Tatil Adı' column with 0 and converts it to a binary indicator (1 if it is a holiday, 0 otherwise).\n",
    "\n",
    "### Steps Involved\n",
    "1. Merge the main dataframe with the weather dataframe.\n",
    "2. Merge the resulting dataframe with the holidays dataframe.\n",
    "3. Sort the dataframe by `ilce`, `year`, `month`, and `day`.\n",
    "4. Create a datetime index from the `year`, `month`, and `day` columns.\n",
    "5. Fill missing holiday names with 0 and create a binary indicator for holidays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(dataframe, weather_df, holidays):\n",
    "    merged_df = pd.merge(dataframe, weather_df, on=['ilce', 'year', 'month', 'day'], how='inner')\n",
    "    final_df = pd.merge(merged_df, holidays, on=['year', 'month', 'day'], how='left')\n",
    "    final_df.sort_values(by=['ilce', 'year', 'month', 'day'], ascending=True, inplace=True)\n",
    "    final_df['tarih'] = pd.to_datetime(final_df[['year', 'month', 'day']])\n",
    "    final_df.set_index('tarih', inplace=True)\n",
    "    final_df['Tatil Adı'].fillna(0, inplace=True)\n",
    "    final_df['Tatil Adı'] = final_df['Tatil Adı'].apply(lambda x: 1 if x != 0 else 0)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Feature Extraction Function\n",
    "\n",
    "This section contains a function to extract various date-related features from the dataframe's index. The function, `extract_date_features`, performs the following operations:\n",
    "\n",
    "1. **Extract Quarter**: Extracts the quarter of the year from the date index.\n",
    "2. **Extract Day of Year**: Extracts the day of the year from the date index.\n",
    "3. **Extract Week of Year**: Extracts the week of the year from the date index.\n",
    "4. **Extract Day of Week**: Extracts the day of the week from the date index.\n",
    "\n",
    "### New Features\n",
    "- `quarter`: The quarter of the year (1 to 4).\n",
    "- `dayofyear`: The day of the year (1 to 365/366).\n",
    "- `weekofyear`: The week of the year (1 to 52/53).\n",
    "- `dayofweek`: The day of the week (0 for Monday to 6 for Sunday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_features(dataframe):\n",
    "\n",
    "    dataframe['quarter'] = dataframe.index.quarter\n",
    "    dataframe['dayofyear'] = dataframe.index.dayofyear\n",
    "    dataframe['weekofyear'] = dataframe.index.weekofyear\n",
    "    dataframe['dayofweek'] = dataframe.index.dayofweek\n",
    "    # dataframe['weekday'] = dataframe.index.weekday\n",
    "    # dataframe['is_Mon'] = np.where(dataframe['dayofweek'] == 1, 1, 0)\n",
    "    # dataframe['is_Tue'] = np.where(dataframe['dayofweek'] == 2, 1, 0)\n",
    "    # dataframe['is_Wed'] = np.where(dataframe['dayofweek'] == 3, 1, 0)\n",
    "    # dataframe['is_Thu'] = np.where(dataframe['dayofweek'] == 4, 1, 0)\n",
    "    # dataframe['is_Fri'] = np.where(dataframe['dayofweek'] == 5, 1, 0)\n",
    "    # dataframe['is_Sat'] = np.where(dataframe['dayofweek'] == 6, 1, 0)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Date Sinusoidal and Cosine Feature Calculation Function\n",
    "\n",
    "This section contains a function to calculate sinusoidal and cosine transformations of various date-related features. The function, `calculate_date_sin_cos`, performs the following operations:\n",
    "\n",
    "1. **Calculate Sinusoidal and Cosine Transformations**: For each date-related feature (month, day of year, day of month, quarter), calculate the sine and cosine values to capture cyclical patterns.\n",
    "\n",
    "### New Features\n",
    "- `sin_month`: The sine transformation of the month.\n",
    "- `cos_month`: The cosine transformation of the month.\n",
    "- `sin_dayofyear`: The sine transformation of the day of the year.\n",
    "- `cos_dayofyear`: The cosine transformation of the day of the year.\n",
    "- `sin_dayofmonth`: The sine transformation of the day of the month.\n",
    "- `cos_dayofmonth`: The cosine transformation of the day of the month.\n",
    "- `sin_quarter`: The sine transformation of the quarter.\n",
    "- `cos_quarter`: The cosine transformation of the quarter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_date_sin_cos(dataframe):\n",
    "    dataframe['sin_month'] = np.sin(2 * np.pi * dataframe['month']/12)\n",
    "    dataframe['cos_month'] = np.cos(2 * np.pi * dataframe['month']/12)\n",
    "    dataframe['sin_dayofyear'] = np.sin(2 * np.pi * dataframe['dayofyear']/365)\n",
    "    dataframe['cos_dayofyear'] = np.cos(2 * np.pi * dataframe['dayofyear']/365)\n",
    "    dataframe['sin_dayofmonth'] = np.sin(2 * np.pi * dataframe['day']/30)\n",
    "    dataframe['cos_dayofmonth'] = np.cos(2 * np.pi * dataframe['day']/30)\n",
    "    dataframe['sin_quarter'] = np.sin(2*np.pi*dataframe.quarter/4)\n",
    "    dataframe['cos_quarter'] = np.cos(2*np.pi*dataframe.quarter/4)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorization Process Function\n",
    "\n",
    "This section contains a function to categorize various weather-related features in the dataframe. The function, `categorize_process`, categorizes specific weather parameters and adds a seasonal category based on the month.\n",
    "\n",
    "### Categorical Features\n",
    "- `season`: Categorizes the month into seasons:\n",
    "  - \"Spring\" for months 3 to 5\n",
    "  - \"Summer\" for months 6 to 8\n",
    "  - \"Autumn\" for months 9 to 11\n",
    "  - \"Winter\" for months 12, 1, and 2\n",
    "\n",
    "### Potential Categorizations (Commented Out)\n",
    "- Wind Speed (`wind_speed_10m:ms`): Categorized into \"Low\", \"Ideal\", \"Pre-high\", and \"High\".\n",
    "- Relative Humidity (`relative_humidity_2m:p`): Categorized into \"Low\", \"Ideal\", \"Pre-high\", and \"High\".\n",
    "- Effective Cloud Cover (`effective_cloud_cover:p`): Categorized into \"Low\", \"Pre-high\", \"Ideal\", and \"High\".\n",
    "- Wind Direction (`wind_dir_10m:d`): Categorized into compass sectors (\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\").\n",
    "- Global Radiation (`global_rad:W`): Categorized into \"Low\", \"Ideal\", \"Pre-High\", and \"High\".\n",
    "- Probability of Precipitation (`prob_precip_1h:p`): Categorized into \"Low\", \"Ideal\", \"Pre-High\", and \"High\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorize_process(dataframe):\n",
    "#     # dataframe.loc[(dataframe[\"wind_speed_10m:ms\"] < 0.5), 'NEW_WIND_CAT'] = 'Low'\n",
    "#     # dataframe.loc[(dataframe[\"wind_speed_10m:ms\"] >= 0.5) & (dataframe[\"wind_speed_10m:ms\"] < 1.5), 'NEW_WIND_CAT'] = 'Ideal'\n",
    "#     # dataframe.loc[(dataframe[\"wind_speed_10m:ms\"] >= 1.5) & (dataframe[\"wind_speed_10m:ms\"] < 2.5), 'NEW_WIND_CAT'] = 'Pre-high'\n",
    "#     # dataframe.loc[(dataframe[\"wind_speed_10m:ms\"] >= 2.5), 'NEW_WIND_CAT'] = 'High'\n",
    "\n",
    "#     # dataframe.loc[(dataframe[\"relative_humidity_2m:p\"] < 35), 'NEW_Humidity_CAT'] = 'Low'\n",
    "#     # dataframe.loc[(dataframe[\"relative_humidity_2m:p\"] >= 35) & (dataframe[\"relative_humidity_2m:p\"] < 55), 'NEW_Humidity_CAT'] = 'Ideal'\n",
    "#     # dataframe.loc[(dataframe[\"relative_humidity_2m:p\"] >= 55) & (dataframe[\"relative_humidity_2m:p\"] < 72), 'NEW_Humidity_CAT'] = 'Pre-high'\n",
    "#     # dataframe.loc[(dataframe[\"relative_humidity_2m:p\"] >= 72), 'NEW_Humidity_CAT'] ='High'\n",
    "\n",
    "#     # dataframe['NEW_CLD_CAT'] = pd.cut(dataframe['effective_cloud_cover:p'], bins=[-1, 25, 50, 75, 101], labels=['Low', 'Pre-high', 'Ideal', 'High'])\n",
    "\n",
    "#     # def wind_direction_category(degrees):\n",
    "#     #     sectors = ['N', 'NE', 'E', 'SE', 'S', 'SW', 'W', 'NW']\n",
    "#     #     index = int((degrees + 22.5) / 45) % 8\n",
    "#     #     return sectors[index]\n",
    "\n",
    "#     # dataframe['wind_direction_sector'] = dataframe['wind_dir_10m:d'].apply(wind_direction_category)\n",
    "#     # dataframe.drop('wind_dir_10m:d', axis=1, inplace=True)\n",
    "    \n",
    "#     # def categorize_solar_radiation(global_rad_mean):\n",
    "#     #     if global_rad_mean <= 50:\n",
    "#     #         return \"Low\"\n",
    "#     #     elif 50 < global_rad_mean <= 200:\n",
    "#     #         return \"Ideal\"\n",
    "#     #     elif 200 < global_rad_mean <= 300:\n",
    "#     #         return \"Pre-High\" \n",
    "#     #     else:\n",
    "#     #         return \"High\"\n",
    "\n",
    "#     # dataframe['NEW_global_rad_CAT'] = dataframe['global_rad:W'].apply(categorize_solar_radiation)\n",
    "\n",
    "#     # def categorize_precipitation(prob_precip_1h):\n",
    "#     #     if prob_precip_1h <= 5:\n",
    "#     #         return \"Low\"\n",
    "#     #     elif 5 < prob_precip_1h <= 15:\n",
    "#     #         return \"Ideal\"\n",
    "#     #     elif 15 < prob_precip_1h <= 25:\n",
    "#     #         return \"Pre-High\"\n",
    "#     #     else:\n",
    "#     #         return \"High\"\n",
    "    \n",
    "#     # dataframe['NEW_precipitation_CAT'] = dataframe['prob_precip_1h:p'].apply(categorize_precipitation)\n",
    "\n",
    "#     def categorize_season(month):\n",
    "#         if 3 <= month <= 5:\n",
    "#             return \"Spring\"\n",
    "#         elif 5 < month <= 8:\n",
    "#             return \"Summer\"\n",
    "#         elif 8 < month <= 11:\n",
    "#             return \"Autumn\" \n",
    "#         else:\n",
    "#             return \"Winter\"\n",
    "\n",
    "#     dataframe['season'] = dataframe['month'].apply(categorize_season)\n",
    "\n",
    "#     return dataframe\n",
    "\n",
    "# train = categorize_process(train)\n",
    "# test = categorize_process(test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save District Data to CSV Function\n",
    "\n",
    "This section contains a function to save data for each district (`ilce`) to separate CSV files. The function, `save_districts_csv`, performs the following operations:\n",
    "\n",
    "1. **Create District Dataframes**: Creates a dictionary of dataframes, one for each district.\n",
    "2. **Save to CSV**: Iterates over each district dataframe and saves it as a CSV file in the specified directory.\n",
    "\n",
    "### Steps Involved\n",
    "1. **Identify Unique Districts**: Extracts unique district names from the dataframe.\n",
    "2. **Create District-specific Dataframes**: Creates a dictionary where each key is a district name, and each value is a dataframe containing data for that district.\n",
    "3. **Save Each District Dataframe to CSV**: Saves each district-specific dataframe to a CSV file named after the district in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_districts_csv(dataframe, dataframe_dir_name):\n",
    "    ilceler = dataframe['ilce'].unique()\n",
    "    ilce_dict = {}\n",
    "    for ilce in ilceler:\n",
    "        ilce_dict[ilce] = dataframe[dataframe['ilce'] == ilce].copy()\n",
    "\n",
    "    for ilce, ilce_df in ilce_dict.items():\n",
    "        name = ilce + \".csv\"\n",
    "        ilce_df.to_csv(rf'C:\\Users\\ahmet\\VSCode\\GDZ-Datathon\\{dataframe_dir_name}\\{name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from Folder Function\n",
    "\n",
    "This section contains a function to load CSV files from a specified folder into a dictionary of dataframes. The function, `load_from_folder`, performs the following operations:\n",
    "\n",
    "1. **List Files in Directory**: Lists all files in the specified folder.\n",
    "2. **Filter CSV Files**: Filters the list to include only CSV files.\n",
    "3. **Load Data**: Loads each CSV file into a dataframe and sets the 'tarih' column as a datetime index.\n",
    "4. **Store in Dictionary**: Stores each dataframe in a dictionary with the district name as the key.\n",
    "\n",
    "### Steps Involved\n",
    "1. **Identify CSV Files**: Lists and filters the files in the specified folder to include only CSV files.\n",
    "2. **Load Dataframes**: Loads each CSV file into a dataframe, converts the 'tarih' column to datetime, and sets it as the index.\n",
    "3. **Store in Dictionary**: Stores each loaded dataframe in a dictionary with the district name (extracted from the file name) as the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_folder(folder_path):\n",
    "    ilce_dataframes = {}\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            ilce = file[:-4] \n",
    "            ilce_df = pd.read_csv(os.path.join(folder_path, file))\n",
    "            ilce_df['tarih'] = pd.to_datetime(ilce_df['tarih'])\n",
    "            ilce_df.set_index('tarih', inplace=True)\n",
    "            ilce_dataframes[ilce] = ilce_df\n",
    "    return ilce_dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Class\n",
    "\n",
    "This section contains a class for extracting additional features from the data. The class, `ExtractFeatures`, provides methods to process both training and testing datasets by adding time-based and differencing features.\n",
    "\n",
    "## Class: `ExtractFeatures`\n",
    "\n",
    "This class is designed to process dataframes by adding new features based on time and differences.\n",
    "\n",
    "##### Parameters\n",
    "- `ilce_train_df`: DataFrame containing the training data.\n",
    "- `ilce_test_df`: DataFrame containing the testing data.\n",
    "- `target_col`: The name of the target column.\n",
    "\n",
    "##### Returns\n",
    "- `ilce_train_df`: The processed training dataframe with new features.\n",
    "- `ilce_test_df`: The processed testing dataframe with new features.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Define Time Features**: Specifies a list of time-related features to include.\n",
    "2. **Identify Numerical Columns**: Identifies numerical columns to use for differencing.\n",
    "3. **Concatenate DataFrames**: Concatenates the training and testing dataframes for consistent feature engineering.\n",
    "4. **Calculate Differences**: Calculates daily and weekly differences for numerical columns.\n",
    "5. **Split DataFrames**: Splits the concatenated dataframe back into training and testing sets.\n",
    "6. **Drop Missing Values**: Drops rows with missing values from the training dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractFeatures:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "    \n",
    "    def process(self, ilce_train_df, ilce_test_df, target_col):\n",
    "        time_features = ['year', 'month', 'day', 'dayofweek' ,'sin_month', 'cos_month', 'sin_dayofmonth', 'cos_dayofmonth', 'Tatil Adı','quarter','dayofyear','weekofyear']\n",
    "        num_cols = [col for col in ilce_train_df.columns if ilce_train_df[col].dtype in ['int64', 'float64'] and col not in [time_features, target_col]]\n",
    "        concatenated_df = pd.concat([ilce_train_df, ilce_test_df], axis=0)\n",
    "\n",
    "        for col in num_cols:\n",
    "            concatenated_df[f'{col}_shift_daily_diff'] = concatenated_df[col] - concatenated_df[col].shift()\n",
    "            concatenated_df[f'{col}_shift_weekly_diff'] = concatenated_df[col] - concatenated_df[col].shift(7)\n",
    "            # concatenated_df[f'{col}_shift_monthly_diff'] = concatenated_df[col] - concatenated_df[col].shift(30)\n",
    "       \n",
    "        ilce_train_df = concatenated_df[:len(ilce_train_df)]\n",
    "        ilce_test_df = concatenated_df[len(ilce_train_df):]\n",
    "        ilce_train_df.dropna(axis=0, inplace=True)\n",
    "\n",
    "        return ilce_train_df, ilce_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scaling Class\n",
    "\n",
    "This section contains a class for scaling the data. The class, `Scaler`, provides methods to scale both training and testing datasets using different scaling techniques based on the configuration.\n",
    "\n",
    "## Class: `Scaler`\n",
    "\n",
    "This class is designed to process dataframes by applying scaling to numerical features.\n",
    "\n",
    "##### Parameters\n",
    "- `ilce_train_df`: DataFrame containing the training data.\n",
    "- `ilce_test_df`: DataFrame containing the testing data.\n",
    "- `target_col`: The name of the target column.\n",
    "\n",
    "##### Returns\n",
    "- `ilce_train_df`: The scaled training dataframe.\n",
    "- `ilce_test_df`: The scaled testing dataframe.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Identify Numerical Columns**: Identifies numerical columns to be scaled, excluding the target column.\n",
    "2. **Select Scaler**: Chooses the appropriate scaler based on the configuration:\n",
    "   - `MinMaxScaler`: Scales features to a given range, typically [0, 1].\n",
    "   - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance.\n",
    "   - `RobustScaler`: Scales features using statistics that are robust to outliers.\n",
    "   - `QuantileTransformer`: Transforms features using quantiles information to map data to a normal distribution.\n",
    "3. **Apply Scaling**: Fits the scaler on the training data and transforms both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scaler:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "    \n",
    "    def process(self, ilce_train_df, ilce_test_df, target_col):\n",
    "        num_cols = [col for col in ilce_train_df.columns if ilce_train_df[col].dtype in ['int64', 'float64'] and col != target_col]\n",
    "\n",
    "        if self.cfg.min_max_scaler:\n",
    "            scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        elif self.cfg.standard_scaler:\n",
    "            scaler = StandardScaler()\n",
    "        elif self.cfg.robust_scaler:\n",
    "            scaler = RobustScaler()\n",
    "        elif self.cfg.quantile_transformer:\n",
    "            scaler = QuantileTransformer(output_distribution='normal')\n",
    "        else:\n",
    "            return ilce_train_df, ilce_test_df\n",
    "\n",
    "        for col in num_cols:\n",
    "            ilce_train_df[col] = scaler.fit_transform(ilce_train_df[[col]])\n",
    "            ilce_test_df[col] = scaler.transform(ilce_test_df[[col]])\n",
    "\n",
    "        return ilce_train_df, ilce_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding Class\n",
    "\n",
    "This section contains a class for encoding categorical features. The class, `Encoder`, provides methods to encode both training and testing datasets using label encoding based on the configuration.\n",
    "\n",
    "## Class: `Encoder`\n",
    "\n",
    "This class is designed to process dataframes by encoding categorical features.\n",
    "\n",
    "##### Parameters\n",
    "- `ilce_train_df`: DataFrame containing the training data.\n",
    "- `ilce_test_df`: DataFrame containing the testing data.\n",
    "- `unique_col`: The unique identifier column that should not be encoded.\n",
    "\n",
    "##### Returns\n",
    "- `ilce_train_df`: The encoded training dataframe.\n",
    "- `ilce_test_df`: The encoded testing dataframe.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Identify Categorical Columns**: Identifies categorical columns to be encoded, excluding the unique identifier column and 'tarih'.\n",
    "2. **Select Encoder**: Chooses the appropriate encoder based on the configuration:\n",
    "   - `LabelEncoder`: Encodes target labels with values between 0 and `n_classes-1`.\n",
    "3. **Apply Encoding**: Fits the encoder on the training data and transforms both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "    \n",
    "    def process(self, ilce_train_df, ilce_test_df, unique_col):\n",
    "        cat_cols = [col for col in ilce_train_df.columns if ilce_train_df[col].dtype in ['object', 'category'] and col not in [unique_col, 'tarih']]\n",
    "\n",
    "        if self.cfg.label_encoder:\n",
    "            encoder = LabelEncoder()\n",
    "            for col in cat_cols:\n",
    "                ilce_train_df[col] = encoder.fit_transform(ilce_train_df[col])\n",
    "                ilce_test_df[col] = encoder.fit_transform(ilce_test_df[col])\n",
    "\n",
    "        return ilce_train_df, ilce_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optuna Hyperparameter Tuning Class\n",
    "\n",
    "This section contains a class for hyperparameter tuning using Optuna. The class, `Optuna`, provides methods to optimize the hyperparameters of various machine learning models using time series cross-validation (TimeSeriesSplit).\n",
    "\n",
    "## Class: `Optuna`\n",
    "\n",
    "This class is designed to perform hyperparameter tuning on different machine learning models using Optuna, which is an automatic hyperparameter optimization framework.\n",
    "\n",
    "Constructor for the `Optuna` class.\n",
    "\n",
    "#### `process(self, dataframe, unique_col, target_col, n_trial)`\n",
    "\n",
    "This method initiates the hyperparameter tuning process for the specified models using the provided data.\n",
    "\n",
    "##### Parameters\n",
    "- `dataframe`: The input dataframe containing the features and target column.\n",
    "- `unique_col`: The unique identifier column to be excluded from features.\n",
    "- `target_col`: The target column for prediction.\n",
    "- `n_trial`: The number of trials for Optuna optimization.\n",
    "\n",
    "##### Returns\n",
    "- `best_params`: A dictionary containing the best parameters and scores for each model.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Drop Columns**: Drops the unique identifier and target columns from the dataframe to create features (X) and target (y).\n",
    "2. **Initialize Best Parameters Dictionary**: Creates a dictionary to store the best parameters for each model.\n",
    "3. **Optimize Models**: Calls the appropriate optimization method for each model based on the configuration flags:\n",
    "   - `histgbr_optuna`: Calls `_optuna_histgbr` to optimize `HistGradientBoostingRegressor`.\n",
    "   - `lgb_optuna`: Calls `_optuna_lgbm` to optimize `LGBMRegressor`.\n",
    "   - `xgb_optuna`: Calls `_optuna_xgb` to optimize `XGBRegressor`.\n",
    "   - `catb_optuna`: Calls `_optuna_catb` to optimize `CatBoostRegressor`.\n",
    "\n",
    "#### `_optuna_histgbr(self, X, y, n_trial)`\n",
    "\n",
    "This private method performs hyperparameter tuning for `HistGradientBoostingRegressor` using Optuna.\n",
    "\n",
    "##### Parameters\n",
    "- `X`: The features dataframe.\n",
    "- `y`: The target series.\n",
    "- `n_trial`: The number of trials for Optuna optimization.\n",
    "\n",
    "##### Returns\n",
    "- `best_params`: The best parameters from the optimization.\n",
    "- `best_value`: The best MAE value from the optimization.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Objective Function**: Defines the objective function for Optuna, which trains the model using the suggested hyperparameters and evaluates it using TimeSeriesSplit cross-validation.\n",
    "2. **Study Creation**: Creates an Optuna study to minimize the objective function.\n",
    "3. **Study Optimization**: Runs the optimization for the specified number of trials.\n",
    "\n",
    "#### `_optuna_lgbm(self, X, y, n_trial)`\n",
    "\n",
    "This private method performs hyperparameter tuning for `LGBMRegressor` using Optuna.\n",
    "\n",
    "##### Parameters\n",
    "- `X`: The features dataframe.\n",
    "- `y`: The target series.\n",
    "- `n_trial`: The number of trials for Optuna optimization.\n",
    "\n",
    "##### Returns\n",
    "- `best_params`: The best parameters from the optimization.\n",
    "- `best_value`: The best MAE value from the optimization.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Objective Function**: Defines the objective function for Optuna, which trains the model using the suggested hyperparameters and evaluates it using TimeSeriesSplit cross-validation.\n",
    "2. **Study Creation**: Creates an Optuna study to minimize the objective function.\n",
    "3. **Study Optimization**: Runs the optimization for the specified number of trials.\n",
    "\n",
    "#### `_optuna_xgb(self, X, y, n_trial)`\n",
    "\n",
    "This private method performs hyperparameter tuning for `XGBRegressor` using Optuna.\n",
    "\n",
    "##### Parameters\n",
    "- `X`: The features dataframe.\n",
    "- `y`: The target series.\n",
    "- `n_trial`: The number of trials for Optuna optimization.\n",
    "\n",
    "##### Returns\n",
    "- `best_params`: The best parameters from the optimization.\n",
    "- `best_value`: The best MAE value from the optimization.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Objective Function**: Defines the objective function for Optuna, which trains the model using the suggested hyperparameters and evaluates it using TimeSeriesSplit cross-validation.\n",
    "2. **Study Creation**: Creates an Optuna study to minimize the objective function.\n",
    "3. **Study Optimization**: Runs the optimization for the specified number of trials.\n",
    "\n",
    "#### `_optuna_catb(self, X, y, n_trial)`\n",
    "\n",
    "This private method performs hyperparameter tuning for `CatBoostRegressor` using Optuna.\n",
    "\n",
    "##### Parameters\n",
    "- `X`: The features dataframe.\n",
    "- `y`: The target series.\n",
    "- `n_trial`: The number of trials for Optuna optimization.\n",
    "\n",
    "##### Returns\n",
    "- `best_params`: The best parameters from the optimization.\n",
    "- `best_value`: The best MAE value from the optimization.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Objective Function**: Defines the objective function for Optuna, which trains the model using the suggested hyperparameters and evaluates it using TimeSeriesSplit cross-validation.\n",
    "2. **Study Creation**: Creates an Optuna study to minimize the objective function.\n",
    "3. **Study Optimization**: Runs the optimization for the specified number of trials.\n",
    "\n",
    "### Example Usage\n",
    "This class can be used to perform hyperparameter tuning on different machine learning models to find the optimal set of parameters for each model, enhancing their predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optuna:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, dataframe, unique_col, target_col, n_trial):\n",
    "        \n",
    "        X = dataframe.drop(columns=[unique_col, target_col])\n",
    "        y = dataframe[target_col]\n",
    "\n",
    "        best_params = {}\n",
    "\n",
    "        if self.cfg.histgbr_optuna:\n",
    "            print(\"Executing HistGradientBoostingRegressor..\")\n",
    "            best_params['histgbr'] = self._optuna_histgbr(X, y, n_trial)\n",
    "\n",
    "        if self.cfg.lgb_optuna:\n",
    "            print(\"Executing LGBMRegressor..\")\n",
    "            best_params['lgbm'] = self._optuna_lgbm(X, y, n_trial)\n",
    "\n",
    "        if self.cfg.xgb_optuna:\n",
    "            print(\"Executing XGBRegressor..\")\n",
    "            best_params['xgb'] = self._optuna_xgb(X, y, n_trial)\n",
    "\n",
    "        if self.cfg.catb_optuna:\n",
    "            print(\"Executing CatBoostRegressor..\")\n",
    "            best_params['catb'] = self._optuna_catb(X, y, n_trial)\n",
    "\n",
    "        return best_params\n",
    "\n",
    "    def _optuna_histgbr(self, X, y, n_trial):\n",
    "        def hist_gradient_objective(trial):\n",
    "            params = {\n",
    "                'loss': 'absolute_error',\n",
    "                'verbose': 0,\n",
    "                'max_iter': trial.suggest_int('max_iter', 100, 2000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "                'max_leaf_nodes': trial.suggest_int('max_leaf_nodes', 2, 128),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 100),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                'l2_regularization': trial.suggest_float('l2_regularization', 1e-5, 1),\n",
    "            }\n",
    "\n",
    "            model = HistGradientBoostingRegressor(**params)\n",
    "            \n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            mae_scores = []\n",
    "\n",
    "            for train_index, test_index in tscv.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mae_scores.append(mae)\n",
    "\n",
    "            return sum(mae_scores) / len(mae_scores)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(hist_gradient_objective, n_trials=n_trial)\n",
    "        return study.best_params, study.best_value\n",
    "\n",
    "    def _optuna_lgbm(self, X, y, n_trial):\n",
    "        def lgbm_objective(trial):\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'mae',\n",
    "                'verbosity': -1,\n",
    "                'boosting_type': 'gbdt',\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 2000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 2, 128),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-5, 1),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-5, 1),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 5, 50)\n",
    "            }\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            mae_scores = []\n",
    "\n",
    "            for train_index, test_index in tscv.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                train_data = lgb.Dataset(X_train, label=y_train)\n",
    "                test_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "                model = lgb.train(params, train_data, valid_sets=[test_data])\n",
    "                y_pred = model.predict(X_test)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "                mae_scores.append(mae)\n",
    "\n",
    "            return sum(mae_scores) / len(mae_scores)\n",
    "            \n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(lgbm_objective, n_trials=n_trial)\n",
    "        return study.best_params, study.best_value\n",
    "\n",
    "    def _optuna_xgb(self, X, y, n_trial):\n",
    "        def xgb_objective(trial):\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'verbosity': 0,\n",
    "                'eval_metric': 'mae',\n",
    "                'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "                'lambda': trial.suggest_loguniform('lambda', 1e-3, 1.0),\n",
    "                'alpha': trial.suggest_loguniform('alpha', 1e-3, 1.0),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10), \n",
    "                'gamma': trial.suggest_float('gamma', 0, 5), \n",
    "                'tree_method': trial.suggest_categorical('tree_method', ['auto', 'exact', 'approx', 'hist', 'gpu_hist']), \n",
    "                'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']), \n",
    "            }\n",
    "\n",
    "            if params['booster'] == 'dart':\n",
    "                params['sample_type'] = trial.suggest_categorical('sample_type', ['uniform', 'weighted'])\n",
    "                params['normalize_type'] = trial.suggest_categorical('normalize_type', ['tree', 'forest'])\n",
    "                params['rate_drop'] = trial.suggest_float('rate_drop', 1e-8, 1.0, log=True)\n",
    "                params['skip_drop'] = trial.suggest_float('skip_drop', 1e-8, 1.0, log=True)\n",
    "\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            mae_scores = []\n",
    "\n",
    "            for train_index, test_index in tscv.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "                dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "                model = xgb.train(params, dtrain)\n",
    "                y_pred = model.predict(dtest)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mae_scores.append(mae)\n",
    "\n",
    "            return sum(mae_scores) / len(mae_scores)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(xgb_objective, n_trials=n_trial)\n",
    "        return study.best_params, study.best_value\n",
    "\n",
    "    def _optuna_catb(self, X, y, n_trial):\n",
    "        def catboost_objective(trial):\n",
    "            params = {\n",
    "                'objective': 'MAE', \n",
    "                'verbose': False,\n",
    "                'eval_metric': 'MAE',\n",
    "                'grow_policy': 'Lossguide',\n",
    "                'iterations': trial.suggest_int('iterations', 100, 2000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),\n",
    "                'depth': trial.suggest_int('depth', 3, 7),  \n",
    "                'l2_leaf_reg': trial.suggest_loguniform('l2_leaf_reg', 1e-3, 1.0),  \n",
    "                'max_leaves': trial.suggest_int('max_leaves', 10, 200),  \n",
    "                'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),  \n",
    "                'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 10.0), \n",
    "                'border_count': trial.suggest_int('border_count', 32, 255),  \n",
    "                'random_strength': trial.suggest_float('random_strength', 0.0, 100.0), \n",
    "            }\n",
    "\n",
    "\n",
    "            model = catb.CatBoostRegressor(**params)\n",
    "            tscv = TimeSeriesSplit(n_splits=5)\n",
    "            mae_scores = []\n",
    "\n",
    "            for train_index, test_index in tscv.split(X):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                mae = mean_absolute_error(y_test, y_pred)\n",
    "                mae_scores.append(mae)\n",
    "\n",
    "            return sum(mae_scores) / len(mae_scores)\n",
    "\n",
    "        study = optuna.create_study(direction='minimize')\n",
    "        study.optimize(catboost_objective, n_trials=n_trial)\n",
    "        return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Models Prediction Class\n",
    "\n",
    "This section contains a class for making predictions using different machine learning models. The class, `MLModels`, provides methods to fit and predict using the best model and parameters identified through hyperparameter tuning.\n",
    "\n",
    "## Class: `MLModels`\n",
    "\n",
    "This class is designed to fit and predict using various machine learning models, including HistGradientBoostingRegressor, LGBMRegressor, XGBRegressor, and CatBoostRegressor, based on the best parameters found during hyperparameter tuning.\n",
    "\n",
    "Constructor for the `MLModels` class.\n",
    "\n",
    "#### `process(self, ilce, ilce_train_df, ilce_test_df, unique_col, target_col, best_model, best_params)`\n",
    "\n",
    "This method fits the best model using the training data and makes predictions on the test data.\n",
    "\n",
    "##### Parameters\n",
    "- `ilce`: The district identifier.\n",
    "- `ilce_train_df`: DataFrame containing the training data.\n",
    "- `ilce_test_df`: DataFrame containing the testing data.\n",
    "- `unique_col`: The unique identifier column to be excluded from features.\n",
    "- `target_col`: The target column for prediction.\n",
    "- `best_model`: The best model identified during hyperparameter tuning.\n",
    "- `best_params`: The best parameters for the best model.\n",
    "\n",
    "##### Returns\n",
    "- `predictions`: A dictionary containing the predictions for the test data.\n",
    "\n",
    "##### Steps Involved\n",
    "1. **Prepare Training and Testing Data**: Drops the unique identifier and target columns from the dataframes to create features (X) and target (y) for both training and testing datasets.\n",
    "2. **Model Selection and Prediction**: Based on the `best_model` parameter, initializes, fits, and predicts using the corresponding model:\n",
    "   - **HistGradientBoostingRegressor**: Fits the model using `HistGradientBoostingRegressor` and makes predictions on the test data.\n",
    "   - **LGBMRegressor**: Fits the model using `LGBMRegressor` and makes predictions on the test data.\n",
    "   - **XGBRegressor**: Fits the model using `XGBRegressor` and makes predictions on the test data.\n",
    "   - **CatBoostRegressor**: Fits the model using `CatBoostRegressor` and makes predictions on the test data.\n",
    "3. **Store Predictions**: Stores the predictions in a dictionary with the district identifier as the key.\n",
    "\n",
    "### Example Usage\n",
    "This class can be used to fit and predict using the best machine learning model and parameters identified through hyperparameter tuning, ensuring optimal predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLModels:\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def process(self, ilce, ilce_train_df, ilce_test_df, unique_col, target_col, best_model, best_params):\n",
    "        predictions = {}\n",
    "\n",
    "        X_train = ilce_train_df.drop(columns=[unique_col, target_col])\n",
    "        y_train = ilce_train_df[target_col]\n",
    "        X_test = ilce_test_df.drop(columns=[unique_col, target_col])\n",
    "        \n",
    "        if best_model == 'histgbr':\n",
    "            model = HistGradientBoostingRegressor(**best_params)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            predictions[ilce] = y_pred_test\n",
    "            print(f'{ilce} test data is predicted with HistGradientBoostingRegressor')\n",
    "\n",
    "\n",
    "        elif best_model == 'lgbm':\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            test_data = lgb.Dataset(X_test)\n",
    "            model = lgb.train(best_params, train_data)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            predictions[ilce] = y_pred_test\n",
    "            print(f'{ilce} test data is predicted with LGBMRegressor')\n",
    "\n",
    "        elif best_model == 'xgb':\n",
    "            \n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dtest = xgb.DMatrix(X_test)\n",
    "            model = xgb.train(best_params, dtrain)\n",
    "            y_pred_test = model.predict(dtest)\n",
    "            predictions[ilce] = y_pred_test\n",
    "            print(f'{ilce} test data is predicted with XGBRegressor')\n",
    "\n",
    "        elif best_model == 'catb':\n",
    "            best_params['verbose'] = False\n",
    "            best_params['grow_policy'] = 'Lossguide'\n",
    "            model = catb.CatBoostRegressor(**best_params)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "            predictions[ilce] = y_pred_test\n",
    "            print(f'{ilce} test data is predicted with CatBoostRegressor')\n",
    "\n",
    "        else:\n",
    "            print('No valid model found.')\n",
    "            return predictions\n",
    "\n",
    "\n",
    "        return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
